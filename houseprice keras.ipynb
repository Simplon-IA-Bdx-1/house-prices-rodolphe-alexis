{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import kaggle\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.regularizers import l1\n",
    "\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Columns: 290 entries, Id to SaleCondition_Partial\n",
      "dtypes: float64(3), int64(35), uint8(252)\n",
      "memory usage: 792.9 KB\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('house-price-train.csv')\n",
    "data_test = pd.read_csv('house_test.csv')\n",
    "data_train = pd.get_dummies(data_train)\n",
    "data_test = pd.get_dummies(data_test)\n",
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, input_dim=290, activation=\"relu\", kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               74496     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 74,753\n",
      "Trainable params: 74,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1095 samples, validate on 365 samples\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1095/1095 [==============================] - 2s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/500\n",
      "1095/1095 [==============================] - 0s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/500\n",
      "1095/1095 [==============================] - 0s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/500\n",
      "1095/1095 [==============================] - 0s 285us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/500\n",
      "1095/1095 [==============================] - 0s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/500\n",
      "1095/1095 [==============================] - 0s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/500\n",
      "1095/1095 [==============================] - 0s 377us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/500\n",
      "1095/1095 [==============================] - 0s 374us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/500\n",
      "1095/1095 [==============================] - 0s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/500\n",
      "1095/1095 [==============================] - 0s 302us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/500\n",
      "1095/1095 [==============================] - 0s 294us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/500\n",
      "1095/1095 [==============================] - 0s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/500\n",
      "1095/1095 [==============================] - 0s 338us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/500\n",
      "1095/1095 [==============================] - 0s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/500\n",
      "1095/1095 [==============================] - 0s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/500\n",
      "1095/1095 [==============================] - 0s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/500\n",
      "1095/1095 [==============================] - 0s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/500\n",
      "1095/1095 [==============================] - 0s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/500\n",
      "1095/1095 [==============================] - 0s 251us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/500\n",
      "1095/1095 [==============================] - 0s 252us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/500\n",
      "1095/1095 [==============================] - 0s 263us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/500\n",
      "1095/1095 [==============================] - 0s 267us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/500\n",
      "1095/1095 [==============================] - 0s 257us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/500\n",
      "1095/1095 [==============================] - 0s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/500\n",
      "1095/1095 [==============================] - 0s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/500\n",
      "1095/1095 [==============================] - 1s 484us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/500\n",
      "1095/1095 [==============================] - 0s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/500\n",
      "1095/1095 [==============================] - 0s 300us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/500\n",
      "1095/1095 [==============================] - 0s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/500\n",
      "1095/1095 [==============================] - 1s 508us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/500\n",
      "1095/1095 [==============================] - ETA: 0s - loss: n - 1s 532us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/500\n",
      "1095/1095 [==============================] - 1s 474us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/500\n",
      "1095/1095 [==============================] - 1s 522us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/500\n",
      "1095/1095 [==============================] - 1s 475us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/500\n",
      "1095/1095 [==============================] - 0s 395us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/500\n",
      "1095/1095 [==============================] - 1s 586us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/500\n",
      "1095/1095 [==============================] - 1s 468us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/500\n",
      "1095/1095 [==============================] - 0s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/500\n",
      "1095/1095 [==============================] - 0s 428us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/500\n",
      "1095/1095 [==============================] - 0s 395us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/500\n",
      "1095/1095 [==============================] - 0s 400us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/500\n",
      "1095/1095 [==============================] - 0s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/500\n",
      "1095/1095 [==============================] - 0s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/500\n",
      "1095/1095 [==============================] - 1s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/500\n",
      "1095/1095 [==============================] - 1s 567us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/500\n",
      "1095/1095 [==============================] - 1s 539us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/500\n",
      "1095/1095 [==============================] - 0s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/500\n",
      "1095/1095 [==============================] - 1s 485us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/500\n",
      "1095/1095 [==============================] - 1s 540us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/500\n",
      "1095/1095 [==============================] - 0s 402us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/500\n",
      "1095/1095 [==============================] - 0s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/500\n",
      "1095/1095 [==============================] - 0s 452us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/500\n",
      "1095/1095 [==============================] - 0s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/500\n",
      "1095/1095 [==============================] - 0s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/500\n",
      "1095/1095 [==============================] - 0s 386us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/500\n",
      "1095/1095 [==============================] - 0s 431us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/500\n",
      "1095/1095 [==============================] - 0s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/500\n",
      "1095/1095 [==============================] - 0s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/500\n",
      "1095/1095 [==============================] - 0s 279us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/500\n",
      "1095/1095 [==============================] - 0s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/500\n",
      "1095/1095 [==============================] - 0s 219us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/500\n",
      "1095/1095 [==============================] - 0s 223us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/500\n",
      "1095/1095 [==============================] - 0s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/500\n",
      "1095/1095 [==============================] - 0s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/500\n",
      "1095/1095 [==============================] - 0s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/500\n",
      "1095/1095 [==============================] - 0s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/500\n",
      "1095/1095 [==============================] - 0s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/500\n",
      "1095/1095 [==============================] - 0s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/500\n",
      "1095/1095 [==============================] - 0s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/500\n",
      "1095/1095 [==============================] - 0s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/500\n",
      "1095/1095 [==============================] - 0s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/500\n",
      "1095/1095 [==============================] - 0s 327us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/500\n",
      "1095/1095 [==============================] - 0s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/500\n",
      "1095/1095 [==============================] - 0s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/500\n",
      "1095/1095 [==============================] - 0s 244us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/500\n",
      "1095/1095 [==============================] - 0s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/500\n",
      "1095/1095 [==============================] - 0s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/500\n",
      "1095/1095 [==============================] - 0s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/500\n",
      "1095/1095 [==============================] - 0s 251us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/500\n",
      "1095/1095 [==============================] - 0s 264us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/500\n",
      "1095/1095 [==============================] - 0s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/500\n",
      "1095/1095 [==============================] - 0s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/500\n",
      "1095/1095 [==============================] - 0s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/500\n",
      "1095/1095 [==============================] - 0s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/500\n",
      "1095/1095 [==============================] - 0s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/500\n",
      "1095/1095 [==============================] - 0s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/500\n",
      "1095/1095 [==============================] - 1s 564us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/500\n",
      "1095/1095 [==============================] - 0s 290us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/500\n",
      "1095/1095 [==============================] - 0s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/500\n",
      "1095/1095 [==============================] - 0s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/500\n",
      "1095/1095 [==============================] - 1s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/500\n",
      "1095/1095 [==============================] - 1s 468us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/500\n",
      "1095/1095 [==============================] - 0s 395us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/500\n",
      "1095/1095 [==============================] - 0s 353us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/500\n",
      "1095/1095 [==============================] - 0s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/500\n",
      "1095/1095 [==============================] - 0s 313us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/500\n",
      "1095/1095 [==============================] - 0s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/500\n",
      "1095/1095 [==============================] - 0s 424us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/500\n",
      "1095/1095 [==============================] - 0s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/500\n",
      "1095/1095 [==============================] - 0s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/500\n",
      "1095/1095 [==============================] - 0s 435us/step - loss: nan - val_loss: nan: 0s - l\n",
      "Epoch 110/500\n",
      "1095/1095 [==============================] - 0s 371us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/500\n",
      "1095/1095 [==============================] - 0s 398us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/500\n",
      "1095/1095 [==============================] - 0s 426us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/500\n",
      "1095/1095 [==============================] - 0s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/500\n",
      "1095/1095 [==============================] - 0s 401us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/500\n",
      "1095/1095 [==============================] - 1s 459us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/500\n",
      "1095/1095 [==============================] - 1s 474us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/500\n",
      "1095/1095 [==============================] - 0s 448us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/500\n",
      "1095/1095 [==============================] - 0s 450us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/500\n",
      "1095/1095 [==============================] - 0s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/500\n",
      "1095/1095 [==============================] - 0s 419us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/500\n",
      "1095/1095 [==============================] - 1s 509us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/500\n",
      "1095/1095 [==============================] - 1s 501us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/500\n",
      "1095/1095 [==============================] - 0s 425us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/500\n",
      "1095/1095 [==============================] - 0s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/500\n",
      "1095/1095 [==============================] - 0s 351us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/500\n",
      "1095/1095 [==============================] - 1s 483us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/500\n",
      "1095/1095 [==============================] - 0s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/500\n",
      "1095/1095 [==============================] - 1s 531us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/500\n",
      "1095/1095 [==============================] - 0s 455us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/500\n",
      "1095/1095 [==============================] - 1s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/500\n",
      "1095/1095 [==============================] - 1s 512us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/500\n",
      "1095/1095 [==============================] - 1s 511us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/500\n",
      "1095/1095 [==============================] - 1s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/500\n",
      "1095/1095 [==============================] - 0s 448us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/500\n",
      "1095/1095 [==============================] - 0s 418us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/500\n",
      "1095/1095 [==============================] - 1s 472us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/500\n",
      "1095/1095 [==============================] - 0s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/500\n",
      "1095/1095 [==============================] - 0s 262us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/500\n",
      "1095/1095 [==============================] - 0s 267us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/500\n",
      "1095/1095 [==============================] - 0s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/500\n",
      "1095/1095 [==============================] - 0s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/500\n",
      "1095/1095 [==============================] - 0s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/500\n",
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/500\n",
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/500\n",
      "1095/1095 [==============================] - 0s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/500\n",
      "1095/1095 [==============================] - 0s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/500\n",
      "1095/1095 [==============================] - 0s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/500\n",
      "1095/1095 [==============================] - 0s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/500\n",
      "1095/1095 [==============================] - 0s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/500\n",
      "1095/1095 [==============================] - 0s 215us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/500\n",
      "1095/1095 [==============================] - 0s 252us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/500\n",
      "1095/1095 [==============================] - 0s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/500\n",
      "1095/1095 [==============================] - 0s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/500\n",
      "1095/1095 [==============================] - 0s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/500\n",
      "1095/1095 [==============================] - 0s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/500\n",
      "1095/1095 [==============================] - 0s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/500\n",
      "1095/1095 [==============================] - 0s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/500\n",
      "1095/1095 [==============================] - 0s 231us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/500\n",
      "1095/1095 [==============================] - 0s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/500\n",
      "1095/1095 [==============================] - 0s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/500\n",
      "1095/1095 [==============================] - 0s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/500\n",
      "1095/1095 [==============================] - 0s 219us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/500\n",
      "1095/1095 [==============================] - 0s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/500\n",
      "1095/1095 [==============================] - 0s 239us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/500\n",
      "1095/1095 [==============================] - 0s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/500\n",
      "1095/1095 [==============================] - 0s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/500\n",
      "1095/1095 [==============================] - 0s 257us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/500\n",
      "1095/1095 [==============================] - 0s 297us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/500\n",
      "1095/1095 [==============================] - 0s 342us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/500\n",
      "1095/1095 [==============================] - 0s 295us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/500\n",
      "1095/1095 [==============================] - 0s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/500\n",
      "1095/1095 [==============================] - 0s 249us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/500\n",
      "1095/1095 [==============================] - ETA: 0s - loss: n - 0s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/500\n",
      "1095/1095 [==============================] - 0s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/500\n",
      "1095/1095 [==============================] - 0s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/500\n",
      "1095/1095 [==============================] - 0s 384us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/500\n",
      "1095/1095 [==============================] - 0s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/500\n",
      "1095/1095 [==============================] - 0s 280us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/500\n",
      "1095/1095 [==============================] - 0s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/500\n",
      "1095/1095 [==============================] - 0s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/500\n",
      "1095/1095 [==============================] - 0s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/500\n",
      "1095/1095 [==============================] - 0s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/500\n",
      "1095/1095 [==============================] - 0s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/500\n",
      "1095/1095 [==============================] - 0s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/500\n",
      "1095/1095 [==============================] - 0s 239us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/500\n",
      "1095/1095 [==============================] - 0s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/500\n",
      "1095/1095 [==============================] - 0s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/500\n",
      "1095/1095 [==============================] - 0s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/500\n",
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/500\n",
      "1095/1095 [==============================] - 0s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/500\n",
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/500\n",
      "1095/1095 [==============================] - 0s 287us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/500\n",
      "1095/1095 [==============================] - 0s 239us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/500\n",
      "1095/1095 [==============================] - 0s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/500\n",
      "1095/1095 [==============================] - 0s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/500\n",
      "1095/1095 [==============================] - 0s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/500\n",
      "1095/1095 [==============================] - 0s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/500\n",
      "1095/1095 [==============================] - 0s 257us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/500\n",
      "1095/1095 [==============================] - 0s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/500\n",
      "1095/1095 [==============================] - 0s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/500\n",
      "1095/1095 [==============================] - 0s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/500\n",
      "1095/1095 [==============================] - 0s 289us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/500\n",
      "1095/1095 [==============================] - 0s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/500\n",
      "1095/1095 [==============================] - 0s 223us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/500\n",
      "1095/1095 [==============================] - 0s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/500\n",
      "1095/1095 [==============================] - 0s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/500\n",
      "1095/1095 [==============================] - 0s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/500\n",
      "1095/1095 [==============================] - 0s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/500\n",
      "1095/1095 [==============================] - 0s 238us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/500\n",
      "1095/1095 [==============================] - 0s 238us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/500\n",
      "1095/1095 [==============================] - 0s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/500\n",
      "1095/1095 [==============================] - 0s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/500\n",
      "1095/1095 [==============================] - 0s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/500\n",
      "1095/1095 [==============================] - 0s 221us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/500\n",
      "1095/1095 [==============================] - 0s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/500\n",
      "1095/1095 [==============================] - 0s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/500\n",
      "1095/1095 [==============================] - 0s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/500\n",
      "1095/1095 [==============================] - 0s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/500\n",
      "1095/1095 [==============================] - 0s 220us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/500\n",
      "1095/1095 [==============================] - 0s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/500\n",
      "1095/1095 [==============================] - 0s 268us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/500\n",
      "1095/1095 [==============================] - 0s 300us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/500\n",
      "1095/1095 [==============================] - 1s 462us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/500\n",
      "1095/1095 [==============================] - 0s 378us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/500\n",
      "1095/1095 [==============================] - 0s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/500\n",
      "1095/1095 [==============================] - 0s 279us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/500\n",
      "1095/1095 [==============================] - 0s 231us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/500\n",
      "1095/1095 [==============================] - 0s 283us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/500\n",
      "1095/1095 [==============================] - 0s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/500\n",
      "1095/1095 [==============================] - 0s 286us/step - loss: nan - val_loss: nan\n",
      "Epoch 251/500\n",
      "1095/1095 [==============================] - ETA: 0s - loss: n - 0s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 252/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 253/500\n",
      "1095/1095 [==============================] - 0s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 254/500\n",
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 255/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 256/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 257/500\n",
      "1095/1095 [==============================] - 0s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 258/500\n",
      "1095/1095 [==============================] - 0s 238us/step - loss: nan - val_loss: nan\n",
      "Epoch 259/500\n",
      "1095/1095 [==============================] - 0s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 260/500\n",
      "1095/1095 [==============================] - 0s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 261/500\n",
      "1095/1095 [==============================] - 0s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 262/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 263/500\n",
      "1095/1095 [==============================] - 0s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 264/500\n",
      "1095/1095 [==============================] - 0s 239us/step - loss: nan - val_loss: nan\n",
      "Epoch 265/500\n",
      "1095/1095 [==============================] - 0s 278us/step - loss: nan - val_loss: nan\n",
      "Epoch 266/500\n",
      "1095/1095 [==============================] - 0s 317us/step - loss: nan - val_loss: nan\n",
      "Epoch 267/500\n",
      "1095/1095 [==============================] - 0s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 268/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 269/500\n",
      "1095/1095 [==============================] - 0s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 270/500\n",
      "1095/1095 [==============================] - 0s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 271/500\n",
      "1095/1095 [==============================] - 0s 215us/step - loss: nan - val_loss: nan\n",
      "Epoch 272/500\n",
      "1095/1095 [==============================] - 0s 252us/step - loss: nan - val_loss: nan\n",
      "Epoch 273/500\n",
      "1095/1095 [==============================] - 0s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 274/500\n",
      "1095/1095 [==============================] - 0s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 275/500\n",
      "1095/1095 [==============================] - 0s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 276/500\n",
      "1095/1095 [==============================] - 0s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 277/500\n",
      "1095/1095 [==============================] - 0s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 278/500\n",
      "1095/1095 [==============================] - 0s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 279/500\n",
      "1095/1095 [==============================] - 0s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 280/500\n",
      "1095/1095 [==============================] - 0s 259us/step - loss: nan - val_loss: nan\n",
      "Epoch 281/500\n",
      "1095/1095 [==============================] - 0s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 282/500\n",
      "1095/1095 [==============================] - 0s 257us/step - loss: nan - val_loss: nan\n",
      "Epoch 283/500\n",
      "1095/1095 [==============================] - 0s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 284/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 285/500\n",
      "1095/1095 [==============================] - 0s 249us/step - loss: nan - val_loss: nan\n",
      "Epoch 286/500\n",
      "1095/1095 [==============================] - 0s 209us/step - loss: nan - val_loss: nan\n",
      "Epoch 287/500\n",
      "1095/1095 [==============================] - 0s 255us/step - loss: nan - val_loss: nan\n",
      "Epoch 288/500\n",
      "1095/1095 [==============================] - 0s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 289/500\n",
      "1095/1095 [==============================] - 0s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 290/500\n",
      "1095/1095 [==============================] - 0s 276us/step - loss: nan - val_loss: nan\n",
      "Epoch 291/500\n",
      "1095/1095 [==============================] - 0s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 292/500\n",
      "1095/1095 [==============================] - 0s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 293/500\n",
      "1095/1095 [==============================] - 0s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 294/500\n",
      "1095/1095 [==============================] - 0s 417us/step - loss: nan - val_loss: nan\n",
      "Epoch 295/500\n",
      "1095/1095 [==============================] - 0s 401us/step - loss: nan - val_loss: nan\n",
      "Epoch 296/500\n",
      "1095/1095 [==============================] - 1s 481us/step - loss: nan - val_loss: nan\n",
      "Epoch 297/500\n",
      "1095/1095 [==============================] - 1s 518us/step - loss: nan - val_loss: nan\n",
      "Epoch 298/500\n",
      "1095/1095 [==============================] - 1s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 299/500\n",
      "1095/1095 [==============================] - 1s 568us/step - loss: nan - val_loss: nan\n",
      "Epoch 300/500\n",
      "1095/1095 [==============================] - 1s 468us/step - loss: nan - val_loss: nan\n",
      "Epoch 301/500\n",
      "1095/1095 [==============================] - 1s 474us/step - loss: nan - val_loss: nan\n",
      "Epoch 302/500\n",
      "1095/1095 [==============================] - 0s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 304/500\n",
      "1095/1095 [==============================] - 0s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 305/500\n",
      "1095/1095 [==============================] - 0s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 306/500\n",
      "1095/1095 [==============================] - 0s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 307/500\n",
      "1095/1095 [==============================] - 0s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 308/500\n",
      "1095/1095 [==============================] - 0s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 309/500\n",
      "1095/1095 [==============================] - 0s 266us/step - loss: nan - val_loss: nan\n",
      "Epoch 310/500\n",
      "1095/1095 [==============================] - 0s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 311/500\n",
      "1095/1095 [==============================] - 0s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 312/500\n",
      "1095/1095 [==============================] - 0s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 313/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 314/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 315/500\n",
      "1095/1095 [==============================] - 0s 215us/step - loss: nan - val_loss: nan\n",
      "Epoch 316/500\n",
      "1095/1095 [==============================] - 0s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 317/500\n",
      "1095/1095 [==============================] - 0s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 318/500\n",
      "1095/1095 [==============================] - 0s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 319/500\n",
      "1095/1095 [==============================] - 0s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 320/500\n",
      "1095/1095 [==============================] - 0s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 321/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 322/500\n",
      "1095/1095 [==============================] - 0s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 323/500\n",
      "1095/1095 [==============================] - 0s 223us/step - loss: nan - val_loss: nan\n",
      "Epoch 324/500\n",
      "1095/1095 [==============================] - 0s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 325/500\n",
      "1095/1095 [==============================] - 0s 426us/step - loss: nan - val_loss: nan\n",
      "Epoch 326/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 327/500\n",
      "1095/1095 [==============================] - 0s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 328/500\n",
      "1095/1095 [==============================] - 0s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 329/500\n",
      "1095/1095 [==============================] - 0s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 330/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 331/500\n",
      "1095/1095 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 332/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 333/500\n",
      "1095/1095 [==============================] - 0s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 334/500\n",
      "1095/1095 [==============================] - 0s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 335/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 336/500\n",
      "1095/1095 [==============================] - 0s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 337/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 338/500\n",
      "1095/1095 [==============================] - 0s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 339/500\n",
      "1095/1095 [==============================] - 0s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 340/500\n",
      "1095/1095 [==============================] - 0s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 341/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 342/500\n",
      "1095/1095 [==============================] - 0s 249us/step - loss: nan - val_loss: nan\n",
      "Epoch 343/500\n",
      "1095/1095 [==============================] - 0s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 344/500\n",
      "1095/1095 [==============================] - 0s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 345/500\n",
      "1095/1095 [==============================] - 0s 249us/step - loss: nan - val_loss: nan\n",
      "Epoch 346/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 347/500\n",
      "1095/1095 [==============================] - 0s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 348/500\n",
      "1095/1095 [==============================] - 0s 269us/step - loss: nan - val_loss: nan\n",
      "Epoch 349/500\n",
      "1095/1095 [==============================] - 0s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 350/500\n",
      "1095/1095 [==============================] - 0s 275us/step - loss: nan - val_loss: nan\n",
      "Epoch 351/500\n",
      "1095/1095 [==============================] - 0s 348us/step - loss: nan - val_loss: nan\n",
      "Epoch 352/500\n",
      "1095/1095 [==============================] - 0s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 353/500\n",
      "1095/1095 [==============================] - 0s 259us/step - loss: nan - val_loss: nan\n",
      "Epoch 354/500\n",
      "1095/1095 [==============================] - 0s 319us/step - loss: nan - val_loss: nan\n",
      "Epoch 355/500\n",
      "1095/1095 [==============================] - 0s 254us/step - loss: nan - val_loss: nan\n",
      "Epoch 356/500\n",
      "1095/1095 [==============================] - 0s 294us/step - loss: nan - val_loss: nan\n",
      "Epoch 357/500\n",
      "1095/1095 [==============================] - 0s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 358/500\n",
      "1095/1095 [==============================] - 0s 289us/step - loss: nan - val_loss: nan\n",
      "Epoch 359/500\n",
      "1095/1095 [==============================] - 0s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 360/500\n",
      "1095/1095 [==============================] - 0s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 361/500\n",
      "1095/1095 [==============================] - 0s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 362/500\n",
      "1095/1095 [==============================] - 0s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 363/500\n",
      "1095/1095 [==============================] - 0s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 364/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 365/500\n",
      "1095/1095 [==============================] - 0s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 366/500\n",
      "1095/1095 [==============================] - 0s 256us/step - loss: nan - val_loss: nan\n",
      "Epoch 367/500\n",
      "1095/1095 [==============================] - 0s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 368/500\n",
      "1095/1095 [==============================] - 0s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 369/500\n",
      "1095/1095 [==============================] - 0s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 370/500\n",
      "1095/1095 [==============================] - 0s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 371/500\n",
      "1095/1095 [==============================] - 0s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 372/500\n",
      "1095/1095 [==============================] - 0s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 373/500\n",
      "1095/1095 [==============================] - 0s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 374/500\n",
      "1095/1095 [==============================] - 0s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 375/500\n",
      "1095/1095 [==============================] - 0s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 376/500\n",
      "1095/1095 [==============================] - 0s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 377/500\n",
      "1095/1095 [==============================] - 0s 254us/step - loss: nan - val_loss: nan\n",
      "Epoch 378/500\n",
      "1095/1095 [==============================] - 0s 293us/step - loss: nan - val_loss: nan\n",
      "Epoch 379/500\n",
      "1095/1095 [==============================] - 0s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 380/500\n",
      "1095/1095 [==============================] - 0s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 381/500\n",
      "1095/1095 [==============================] - 0s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 382/500\n",
      "1095/1095 [==============================] - 0s 402us/step - loss: nan - val_loss: nan\n",
      "Epoch 383/500\n",
      "1095/1095 [==============================] - 0s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 384/500\n",
      "1095/1095 [==============================] - 0s 317us/step - loss: nan - val_loss: nan\n",
      "Epoch 385/500\n",
      "1095/1095 [==============================] - 0s 306us/step - loss: nan - val_loss: nan\n",
      "Epoch 386/500\n",
      "1095/1095 [==============================] - 0s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 387/500\n",
      "1095/1095 [==============================] - 0s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 388/500\n",
      "1095/1095 [==============================] - 0s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 389/500\n",
      "1095/1095 [==============================] - 0s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 390/500\n",
      "1095/1095 [==============================] - 0s 244us/step - loss: nan - val_loss: nan\n",
      "Epoch 391/500\n",
      "1095/1095 [==============================] - 0s 298us/step - loss: nan - val_loss: nan\n",
      "Epoch 392/500\n",
      "1095/1095 [==============================] - 0s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 393/500\n",
      "1095/1095 [==============================] - 0s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 394/500\n",
      "1095/1095 [==============================] - 0s 284us/step - loss: nan - val_loss: nan\n",
      "Epoch 395/500\n",
      "1095/1095 [==============================] - 0s 238us/step - loss: nan - val_loss: nan\n",
      "Epoch 396/500\n",
      "1095/1095 [==============================] - 0s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 397/500\n",
      "1095/1095 [==============================] - 0s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 398/500\n",
      "1095/1095 [==============================] - 0s 231us/step - loss: nan - val_loss: nan\n",
      "Epoch 399/500\n",
      "1095/1095 [==============================] - 0s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 400/500\n",
      "1095/1095 [==============================] - 0s 255us/step - loss: nan - val_loss: nan\n",
      "Epoch 401/500\n",
      "1095/1095 [==============================] - 0s 244us/step - loss: nan - val_loss: nan\n",
      "Epoch 402/500\n",
      "1095/1095 [==============================] - 0s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 403/500\n",
      "1095/1095 [==============================] - 0s 268us/step - loss: nan - val_loss: nan\n",
      "Epoch 404/500\n",
      "1095/1095 [==============================] - 0s 304us/step - loss: nan - val_loss: nan\n",
      "Epoch 405/500\n",
      "1095/1095 [==============================] - 1s 633us/step - loss: nan - val_loss: nan\n",
      "Epoch 406/500\n",
      "1095/1095 [==============================] - 0s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 407/500\n",
      "1095/1095 [==============================] - 0s 382us/step - loss: nan - val_loss: nan\n",
      "Epoch 408/500\n",
      "1095/1095 [==============================] - 0s 393us/step - loss: nan - val_loss: nan\n",
      "Epoch 409/500\n",
      "1095/1095 [==============================] - 0s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 410/500\n",
      "1095/1095 [==============================] - 0s 396us/step - loss: nan - val_loss: nan\n",
      "Epoch 411/500\n",
      "1095/1095 [==============================] - 0s 387us/step - loss: nan - val_loss: nan\n",
      "Epoch 412/500\n",
      "1095/1095 [==============================] - 0s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 413/500\n",
      "1095/1095 [==============================] - 0s 421us/step - loss: nan - val_loss: nan\n",
      "Epoch 414/500\n",
      "1095/1095 [==============================] - 0s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 415/500\n",
      "1095/1095 [==============================] - 0s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 416/500\n",
      "1095/1095 [==============================] - 0s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 417/500\n",
      "1095/1095 [==============================] - 0s 301us/step - loss: nan - val_loss: nan\n",
      "Epoch 418/500\n",
      "1095/1095 [==============================] - 0s 268us/step - loss: nan - val_loss: nan\n",
      "Epoch 419/500\n",
      "1095/1095 [==============================] - 0s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 420/500\n",
      "1095/1095 [==============================] - 0s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 421/500\n",
      "1095/1095 [==============================] - 0s 295us/step - loss: nan - val_loss: nan\n",
      "Epoch 422/500\n",
      "1095/1095 [==============================] - 0s 306us/step - loss: nan - val_loss: nan\n",
      "Epoch 423/500\n",
      "1095/1095 [==============================] - 0s 351us/step - loss: nan - val_loss: nan\n",
      "Epoch 424/500\n",
      "1095/1095 [==============================] - 0s 416us/step - loss: nan - val_loss: nan\n",
      "Epoch 425/500\n",
      "1095/1095 [==============================] - 0s 379us/step - loss: nan - val_loss: nan\n",
      "Epoch 426/500\n",
      "1095/1095 [==============================] - 0s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 427/500\n",
      "1095/1095 [==============================] - 0s 373us/step - loss: nan - val_loss: nan\n",
      "Epoch 428/500\n",
      "1095/1095 [==============================] - 0s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 429/500\n",
      "1095/1095 [==============================] - 0s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 430/500\n",
      "1095/1095 [==============================] - 0s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 431/500\n",
      "1095/1095 [==============================] - 0s 381us/step - loss: nan - val_loss: nan\n",
      "Epoch 432/500\n",
      "1095/1095 [==============================] - 0s 300us/step - loss: nan - val_loss: nan\n",
      "Epoch 433/500\n",
      "1095/1095 [==============================] - 0s 342us/step - loss: nan - val_loss: nan\n",
      "Epoch 434/500\n",
      "1095/1095 [==============================] - 0s 393us/step - loss: nan - val_loss: nan\n",
      "Epoch 435/500\n",
      "1095/1095 [==============================] - 0s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 436/500\n",
      "1095/1095 [==============================] - 0s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 437/500\n",
      "1095/1095 [==============================] - 0s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 438/500\n",
      "1095/1095 [==============================] - 0s 361us/step - loss: nan - val_loss: nan\n",
      "Epoch 439/500\n",
      "1095/1095 [==============================] - 1s 462us/step - loss: nan - val_loss: nan\n",
      "Epoch 440/500\n",
      "1095/1095 [==============================] - 0s 317us/step - loss: nan - val_loss: nan\n",
      "Epoch 441/500\n",
      "1095/1095 [==============================] - 0s 364us/step - loss: nan - val_loss: nan\n",
      "Epoch 442/500\n",
      "1095/1095 [==============================] - 0s 429us/step - loss: nan - val_loss: nan\n",
      "Epoch 443/500\n",
      "1095/1095 [==============================] - 0s 429us/step - loss: nan - val_loss: nan\n",
      "Epoch 444/500\n",
      "1095/1095 [==============================] - 1s 472us/step - loss: nan - val_loss: nan\n",
      "Epoch 445/500\n",
      "1095/1095 [==============================] - 1s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 446/500\n",
      "1095/1095 [==============================] - 1s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 447/500\n",
      "1095/1095 [==============================] - 1s 457us/step - loss: nan - val_loss: nan\n",
      "Epoch 448/500\n",
      "1095/1095 [==============================] - 1s 463us/step - loss: nan - val_loss: nan\n",
      "Epoch 449/500\n",
      "1095/1095 [==============================] - 1s 476us/step - loss: nan - val_loss: nan\n",
      "Epoch 450/500\n",
      "1095/1095 [==============================] - 0s 372us/step - loss: nan - val_loss: nan\n",
      "Epoch 451/500\n",
      "1095/1095 [==============================] - 0s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 452/500\n",
      "1095/1095 [==============================] - 0s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 453/500\n",
      "1095/1095 [==============================] - 0s 450us/step - loss: nan - val_loss: nan\n",
      "Epoch 454/500\n",
      "1095/1095 [==============================] - 0s 422us/step - loss: nan - val_loss: nan\n",
      "Epoch 455/500\n",
      "1095/1095 [==============================] - 0s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 456/500\n",
      "1095/1095 [==============================] - 0s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 457/500\n",
      "1095/1095 [==============================] - ETA: 0s - loss: n - 0s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 458/500\n",
      "1095/1095 [==============================] - 0s 419us/step - loss: nan - val_loss: nan\n",
      "Epoch 459/500\n",
      "1095/1095 [==============================] - 0s 370us/step - loss: nan - val_loss: nan\n",
      "Epoch 460/500\n",
      "1095/1095 [==============================] - 1s 486us/step - loss: nan - val_loss: nan\n",
      "Epoch 461/500\n",
      "1095/1095 [==============================] - 0s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 462/500\n",
      "1095/1095 [==============================] - 1s 481us/step - loss: nan - val_loss: nan\n",
      "Epoch 463/500\n",
      "1095/1095 [==============================] - 0s 415us/step - loss: nan - val_loss: nan\n",
      "Epoch 464/500\n",
      "1095/1095 [==============================] - 1s 464us/step - loss: nan - val_loss: nan\n",
      "Epoch 465/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 285us/step - loss: nan - val_loss: nan\n",
      "Epoch 466/500\n",
      "1095/1095 [==============================] - 0s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 467/500\n",
      "1095/1095 [==============================] - 0s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 468/500\n",
      "1095/1095 [==============================] - 0s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 469/500\n",
      "1095/1095 [==============================] - 0s 244us/step - loss: nan - val_loss: nan\n",
      "Epoch 470/500\n",
      "1095/1095 [==============================] - 0s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 471/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 472/500\n",
      "1095/1095 [==============================] - 0s 239us/step - loss: nan - val_loss: nan\n",
      "Epoch 473/500\n",
      "1095/1095 [==============================] - 0s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 474/500\n",
      "1095/1095 [==============================] - 0s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 475/500\n",
      "1095/1095 [==============================] - 0s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 476/500\n",
      "1095/1095 [==============================] - 0s 348us/step - loss: nan - val_loss: nan\n",
      "Epoch 477/500\n",
      "1095/1095 [==============================] - 0s 305us/step - loss: nan - val_loss: nan\n",
      "Epoch 478/500\n",
      "1095/1095 [==============================] - 0s 374us/step - loss: nan - val_loss: nan\n",
      "Epoch 479/500\n",
      "1095/1095 [==============================] - 0s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 480/500\n",
      "1095/1095 [==============================] - 0s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 481/500\n",
      "1095/1095 [==============================] - 0s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 482/500\n",
      "1095/1095 [==============================] - 0s 292us/step - loss: nan - val_loss: nan\n",
      "Epoch 483/500\n",
      "1095/1095 [==============================] - 0s 299us/step - loss: nan - val_loss: nan\n",
      "Epoch 484/500\n",
      "1095/1095 [==============================] - 0s 450us/step - loss: nan - val_loss: nan\n",
      "Epoch 485/500\n",
      "1095/1095 [==============================] - 1s 560us/step - loss: nan - val_loss: nan\n",
      "Epoch 486/500\n",
      "1095/1095 [==============================] - 0s 377us/step - loss: nan - val_loss: nan\n",
      "Epoch 487/500\n",
      "1095/1095 [==============================] - 1s 475us/step - loss: nan - val_loss: nan\n",
      "Epoch 488/500\n",
      "1095/1095 [==============================] - 0s 313us/step - loss: nan - val_loss: nan\n",
      "Epoch 489/500\n",
      "1095/1095 [==============================] - 0s 382us/step - loss: nan - val_loss: nan\n",
      "Epoch 490/500\n",
      "1095/1095 [==============================] - 1s 483us/step - loss: nan - val_loss: nan\n",
      "Epoch 491/500\n",
      "1095/1095 [==============================] - 0s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 492/500\n",
      "1095/1095 [==============================] - 0s 354us/step - loss: nan - val_loss: nan\n",
      "Epoch 493/500\n",
      "1095/1095 [==============================] - 0s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 494/500\n",
      "1095/1095 [==============================] - 0s 353us/step - loss: nan - val_loss: nan\n",
      "Epoch 495/500\n",
      "1095/1095 [==============================] - 0s 437us/step - loss: nan - val_loss: nan\n",
      "Epoch 496/500\n",
      "1095/1095 [==============================] - 0s 269us/step - loss: nan - val_loss: nan\n",
      "Epoch 497/500\n",
      "1095/1095 [==============================] - 0s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 498/500\n",
      "1095/1095 [==============================] - 0s 267us/step - loss: nan - val_loss: nan\n",
      "Epoch 499/500\n",
      "1095/1095 [==============================] - 0s 255us/step - loss: nan - val_loss: nan\n",
      "Epoch 500/500\n",
      "1095/1095 [==============================] - 0s 256us/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2008c04c7c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = StandardScaler().fit_transform(data_train)\n",
    "y = data_train.SalePrice\n",
    "\n",
    "# Split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)\n",
    "X_tr.shape\n",
    "X_tr\n",
    "#Model\n",
    "model = Sequential()\n",
    "\n",
    "# ANN\n",
    "model.add(Dense(256, activation=\"relu\", input_dim = X_train.shape[1],\n",
    "                     kernel_initializer=\"uniform\"))\n",
    "model.add(Dense(1, input_dim = X_train.shape[1], W_regularizer=l1(0.001),activation=\"relu\",\n",
    "                     kernel_initializer=\"uniform\"))\n",
    "# Compile\n",
    "model.compile(loss = \"mse\", optimizer = \"adam\")\n",
    "\n",
    "\n",
    "model.summary()\n",
    "hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 500)\n",
    "\n",
    "pd.Series(model.predict(X_val)[:,0]).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot \n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "## Keras Model\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datasets\n",
    "\n",
    "train = pd.read_csv(\"house-price-train.csv\")\n",
    "test = pd.read_csv(\"house_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in log1p\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAF1CAYAAAAA6ZfwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df7RdZX3n8fe3RAERCT9CGpNosMQs7KQiZhCHmfZWrOWHY3AKU2iWBIwrrhZbXWZVg3b1x0w7A9NlQWoHzRJt7KjIoBQKWKWBMx07AxYUAY0MkYnkDpGo/JAr1Znod/44z4Xj9dx7T3LPj33zvF9rnXX2fvaz9/7cc87d53v3fc4+kZlIkiRJNfuZUQeQJEmSRs2iWJIkSdWzKJYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdWzKFZPImJnRLy2T9t6XUT89RzWXxcRn+9HlkGLiM9ExOmjziFJ81VE/KuIeGDUOXTgC69TrF5ExE7gLZn5d33Y1l3A2zLzjjkHG7GIWAJ8CFgDLAGOy8ydHctPBq7KzFeOJqEkSeqFZ4o1VBHxz4Ej9rcgjogFfY403X52RsSKHrr+GPhb4Ne6LczMLwIviIg1/UsnSXUY1jFfAoti7YeIODgiroiIR8rtiog4uGP5uyJid1n2lojIiDi+LD4D+G9TtpcR8TsR8VBEfCci/jQifqYsuzAi/iEiLo+Ix4A/LG1f6Fj/5yPi1oh4LCIejYj3lPafiYjNEfGNiPhuRFwbEUf187HIzEcz8z8D/zhDtxZwVj/3K0nzWTnxcElEfC0iHo+Ij0bEIRExFhHjEfHuiPgW8NHJto51l5ehad8ux/YPdCx7c0RsL9v8XES8eCQ/oOYli2Ltj/cCpwAnAi8HTgZ+D6CMn30n8FrgeOCXpqy7Gug2NuyNtIcgnASsBd7csexVwEPAscCfdK4UEYcDf0f7bO0Lyz63lcW/A5xdMrwQeBz4i338WfthO+3HSZL0rHXArwI/B7yU8j4C/CxwFPBiYGPnChFxEHAT8E1gBbAUuKYsOxt4D/BvgEXAfwc+OeCfQQcQi2Ltj3XAv8vMPZn5beCPgDeVZf8W+GhmfjUzny7LOi0Enuqyzcsy87HMfBi4Aji/Y9kjmfnnmbk3M/9pynqvB76Vme/LzB9k5lOZeWdZ9lbgvZk5npk/BP4QOGcE/457ivbPLUl61gcyc1dmPkb7hMfkcf/HwB9k5g+7HPNPpn2S43cz8/vluD/5n8O3Av8xM7dn5l7gPwAnerZYvbIo1v54Ie2/0id9s7RNLtvVsaxzGtpnaw/vss3Ofp3b67aNTsuBb0yz7MXA9RHxREQ8QfuM7Y+AxVM7RsSLJvuVvi8C7u1o+40ZMszmcOCJOawvSQei6Y77387MH0yzznLgm6XonerFwPs7juOPAUH7bLI0K4ti7Y9HaB98Jr2otAHsBpZ1LFs+Zd17af+bbKrOfp3bA5jpEim7aP/rbbplZ2Tmwo7bIZn5f6Z2zMyHO/sBDwO/0NH2iRkyzOYE4CtzWF+SDkTTHfdnO+a/aJr/+O0C3jrlmH9oZv6PPuXVAc6iWPvjk8DvRcSiiDgG+H3gv5Rl1wIXRcQJEfG8sqzTLfz0OGOA342IIyNiOfB24FM9ZrkJ+NmIeEf5AODhEfGqsuyDwJ9M/uus5F3b80/Zo4g4BJj8oOHBZb7TLwGf7fd+JWmeuzgilpUPQL+H3o77X6R98uXSiDisfDjv1LLsg8AlEfHzABFxREScO5DkOiBZFGt//DFwF+2zvvcBXyptZOZngSuB24EdwP8s6/ywLP8S8GRH4TrpBuBu4B7gZuDqXoJk5lPArwD/GvgW8CDwy2Xx+4Ebgc9HxFPAHbQ/tNdv/wRMlOmvl3ngmUvQfb9cmk2S9KxPAJ+n/UHqhyjvIzPJzB/RPt4fT/s/euPAr5dl1wOXAddExPeA+2lf8UjqiV/eoYGKiBNoH5gOnhwDFhGvA34rM88u8wmszMwdo0s6GBHxaeDqzLxl1FkkqSn6+YVQUr94UWz1XUS8kfbZ3sNo/9X+N50fisjMz9M+O3DAy8yuX+ohSZKaxeETGoS3At+mfVWIHwG/Odo4kiRJM3P4hCRJkqrnmWJJkiRVz6JYkiRJ1WvEB+2OOeaYXLFixZy28f3vf5/DDjusP4EGzKyDYdbBqCnr3Xff/Z3MXNTHSOqwcOHCPP7440cd4yc07fXdtDxgpl41LVPT8kBzMk17rM/Mkd9e+cpX5lzdfvvtc97GsJh1MMw6GDVlBe7KBhwTD9TbS1/60n14Noajaa/vpuXJNFOvmpapaXkym5NpumO9wyckSZJUPYtiSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUvQWjDqAD24rNNw9tX395+mFD25ekA8swj1U7Lz1raPuS1DvPFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJFUuIlZFxD0dt+9FxDsi4qiIuDUiHiz3R5b+ERFXRsSOiLg3Ik4a9c8gSXNlUSxJlcvMBzLzxMw8EXgl8DRwPbAZ2JaZK4FtZR7gDGBluW0Erhp+aknqL4tiSVKn04BvZOY3gbXA1tK+FTi7TK8FPpZtdwALI2LJ8KNKUv/4jXaSpE7nAZ8s04szczdAZu6OiGNL+1JgV8c646Vt99SNRcRG2meTWbRoEa1Wa0Cx98/ExAStVotNq/cObZ8zPQaTeZrETL1pWqam5YFmZupkUSxJAiAingu8Abhktq5d2rJbx8zcAmwBWLVqVY6Njc0lYt+1Wi3Gxsa4cJhf87xubNY8TWKm3jQtU9PyQDMzdXL4hCRp0hnAlzLz0TL/6OSwiHK/p7SPA8s71lsGPDK0lJI0ABbFkqRJ5/Ps0AmAG4H1ZXo9cENH+wXlKhSnAE9ODrOQpPnK4ROSJCLiecCvAG/taL4UuDYiNgAPA+eW9luAM4EdtK9UcdEQo0rSQFgUS5LIzKeBo6e0fZf21Sim9k3g4iFFk6ShcPiEJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6PRfFEXFQRHw5Im4q88dFxJ0R8WBEfCoinlvaDy7zO8ryFYOJLkmSJPXHvpwpfjuwvWP+MuDyzFwJPA5sKO0bgMcz83jg8tJPkiRJaqyeiuKIWAacBXy4zAfwGuC60mUrcHaZXlvmKctPK/0lSZKkRlrQY78rgHcBh5f5o4EnMnNvmR8HlpbppcAugMzcGxFPlv7f6dxgRGwENgIsXryYVqu1nz9C28TExJy3MSw1Zd20eu/snfpkz2NP8ucfv2Hg+1m99Ig5b6Om18AwzaeskqRmmbUojojXA3sy8+6IGJts7tI1e1j2bEPmFmALwJo1a3JsbGxql33SarWY6zaGpaasF26+uX9hZrFp9V7ed1+vf+ftv53rxua8jZpeA8M0n7JKkpqllwriVOANEXEmcAjwAtpnjhdGxIJytngZ8EjpPw4sB8YjYgFwBPBY35NLkiRJfTLrmOLMvCQzl2XmCuA84LbMXAfcDpxTuq0HJv9vfWOZpyy/LTN/6kyxJEmS1BRzuU7xu4F3RsQO2mOGry7tVwNHl/Z3ApvnFlGSNGgRsTAirouIr0fE9oh4dUQcFRG3lktv3hoRR5a+ERFXlktv3hsRJ406vyTN1T4NwMzMFtAq0w8BJ3fp8wPg3D5kkyQNz/uBv83Mc8p1558HvAfYlpmXRsRm2ic53g2cAawst1cBV5V7SZq3/EY7SapcRLwA+EXKf/wy8/9m5hP85CU2p15682PZdgftz5gsGXJsSeqrwX9UX5LUdC8Bvg18NCJeDtxN+wubFmfmboDM3B0Rx5b+z1x6s5i8LOfuqRvuvPzmokWLGnfJvMnL+A3z8pEzPQZNvKygmXrTtExNywPNzNTJoliStAA4CfjtzLwzIt7PzJ8H6enSm/CTl99ctWrVnC+/2W+Tl/Eb5uUjZ7qsYxMvK2im3jQtU9PyQDMzdXL4hCRpHBjPzDvL/HW0i+RHJ4dFlPs9Hf2Xd6zfeVlOSZqXLIolqXKZ+S1gV0SsKk2nAV/jJy+xOfXSmxeUq1CcAjw5OcxCkuYrh09IkgB+G/h4ufLEQ8BFtE+cXBsRG4CHefbKQrcAZwI7gKdLX0ma1yyKJUlk5j3Ami6LTuvSN4GLBx5KkobI4ROSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEkiInZGxH0RcU9E3FXajoqIWyPiwXJ/ZGmPiLgyInZExL0RcdJo00vS3FkUS5Im/XJmnpiZa8r8ZmBbZq4EtpV5gDOAleW2Ebhq6Eklqc8siiVJ01kLbC3TW4GzO9o/lm13AAsjYskoAkpSvywYdQBJUiMk8PmISOBDmbkFWJyZuwEyc3dEHFv6LgV2daw7Xtp2T91oRGykfTaZRYsW0Wq1BvcT7IeJiQlarRabVu8d2j5negwm8zSJmXrTtExNywPNzNTJoliSBHBqZj5SCt9bI+LrM/SNLm3ZrWMprrcArFq1KsfGxuYctJ9arRZjY2NcuPnmoe1z57qxWfM0iZl607RMTcsDzczUyeETkiQy85Fyvwe4HjgZeHRyWES531O6jwPLO1ZfBjwyvLSS1H8WxZJUuYg4LCIOn5wGXgfcD9wIrC/d1gM3lOkbgQvKVShOAZ6cHGYhSfOVwyckSYuB6yMC2u8Ln8jMv42IfwSujYgNwMPAuaX/LcCZwA7gaeCi4UeWpP6yKJakymXmQ8DLu7R/FzitS3sCFw8hmiQNjcMnJEmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVD2LYkmSJFXPoliSJEnVm7UojohDIuKLEfGViPhqRPxRaT8uIu6MiAcj4lMR8dzSfnCZ31GWrxjsjyBJkiTNTS9nin8IvCYzXw6cCJxevtbzMuDyzFwJPA5sKP03AI9n5vHA5aWfJEmS1FizFsXZNlFmn1NuCbwGuK60bwXOLtNryzxl+WlRvjtUkiRJaqKexhRHxEERcQ+wB7gV+AbwRGbuLV3GgaVleimwC6AsfxI4up+hJUmSpH5a0EunzPwRcGJELASuB07o1q3cdzsrnFMbImIjsBFg8eLFtFqtXqJMa2JiYs7bGJaasm5avXf2Tn2y+NDh7K8fz11Nr4Fhmk9ZJUnN0lNRPCkzn4iIFnAKsDAiFpSzwcuAR0q3cWA5MB4RC4AjgMe6bGsLsAVgzZo1OTY2tr8/A9AuVOa6jWGpKeuFm2/uX5hZbFq9l/fdt08v6f2yc93YnLdR02tgmOZTVklSs/Ry9YlF5QwxEXEo8FpgO3A7cE7pth64oUzfWOYpy2/LzJ86UyxJkiQ1RS+n1ZYAWyPiINpF9LWZeVNEfA24JiL+GPgycHXpfzXwVxGxg/YZ4vMGkFuSJEnqm1mL4sy8F3hFl/aHgJO7tP8AOLcv6SRJkqQh8BvtJEmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVD2LYkmSJFXPoliSBEBEHBQRX46Im8r8cRFxZ0Q8GBGfiojnlvaDy/yOsnzFKHNLUj9YFEuSJr0d2N4xfxlweWauBB4HNpT2DcDjmXk8cHnpJ0nzmkWxJImIWAacBXy4zAfwGuC60mUrcHaZXlvmKctPK/0lad5aMOoAkqRGuAJ4F3B4mT8aeCIz95b5cWBpmV4K7ALIzL0R8WTp/52pG42IjcBGgEWLFtFqtQaVf79MTEzQarXYtHrv7J37ZKbHYDJPk5ipN03L1LQ80MxMnSyKJalyEfF6YE9m3h0RY5PNXbpmD8t+sjFzC7AFYNWqVTk2Ntat28i0Wi3Gxsa4cPPNQ9vnznVjs+ZpEjP1pmmZmpYHmpmpk0WxJOlU4A0RcSZwCPAC2meOF0bEgnK2eBnwSOk/DiwHxiNiAXAE8NjwY0tS/zimWJIql5mXZOayzFwBnAfclpnrgNuBc0q39cANZfrGMk9Zfltmdj1TLEnzhUWxJGk67wbeGRE7aI8Zvrq0Xw0cXdrfCWweUT5J6huHT0iSnpGZLaBVph8CTu7S5wfAuUMNJkkD5pliSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdXzkmySJA3Rihm+UnrT6r19+8rpnZee1ZftSLXwTLEkSZKqZ1EsSZKk6lkUS5IkqXoWxZIkSaqeRbEkSZKqZ1EsSZKk6lkUS5IkqXoWxZIkSaqeRbEkSZKqZ1EsSZKk6lkUS5IkqXoWxZIkSaqeRbEkSZKqt2DUAaT5ZsXmm+e8jU2r93JhD9vZeelZc96XJEmanWeKJUmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVD2LYkmSJFXPoliSJEnVsyiWJElS9SyKJUmSVL1Zi+KIWB4Rt0fE9oj4akS8vbQfFRG3RsSD5f7I0h4RcWVE7IiIeyPipEH/EJIkSdJc9HKmeC+wKTNPAE4BLo6IlwGbgW2ZuRLYVuYBzgBWlttG4Kq+p5YkSZL6aNaiODN3Z+aXyvRTwHZgKbAW2Fq6bQXOLtNrgY9l2x3AwohY0vfkkiRJUp/s05jiiFgBvAK4E1icmbuhXTgDx5ZuS4FdHauNlzZJUkNFxCER8cWI+EoZKvdHpf24iLizDJX7VEQ8t7QfXOZ3lOUrRplfkuZqQa8dI+L5wKeBd2Tm9yJi2q5d2rLL9jbSHl7B4sWLabVavUbpamJiYs7bGJaasm5avbd/YWax+NDh7m8ues3ahNdJTa/Xyv0QeE1mTkTEc4AvRMRngXcCl2fmNRHxQWAD7WFxG4DHM/P4iDgPuAz49VGFl6S56qkoLgfITwMfz8zPlOZHI2JJZu4uwyP2lPZxYHnH6suAR6ZuMzO3AFsA1qxZk2NjY/v3ExStVou5bmNYasp64eab+xdmFptW7+V99/X8d95I9Zp157qxwYeZRU2v15plZgITZfY55ZbAa4DfKO1bgT+kXRSvLdMA1wEfiIgo25GkeaeXq08EcDWwPTP/rGPRjcD6Mr0euKGj/YJyFYpTgCcnh1lIkporIg6KiHton+S4FfgG8ERmTv5bo3M43DND5cryJ4Gjh5tYkvqnl9NqpwJvAu4rB0uA9wCXAtdGxAbgYeDcsuwW4ExgB/A0cFFfE0uSBiIzfwScGBELgeuBE7p1K/f7PFRu0aJFjRveMjnkpilDr/o5DKxfj3UThyWZaXZNywPNzNRp1qI4M79A94MfwGld+idw8RxzSZJGJDOfiIgW7ctwLoyIBeVscOdwuMmhcuMRsQA4Anisy7aeGSq3atWqOQ+V67fJITfDHOo1k34OA+vX8KsmDksy0+yalgeamamT32gnSSIiFpUzxETEocBraV+C83bgnNJt6lC5ySF05wC3OZ5Y0nw2Pz6VJEkatCXA1og4iPYJk2sz86aI+BpwTUT8MfBl2p8xodz/VUTsoH2G+LxRhJakfrEoliSRmffSvg791PaHgJO7tP+AZz9LIknznsMnJEmSVD2LYkmSJFXP4ROVWtHjJ603rd7bmE9lS5IkDYpniiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMolqTKRcTyiLg9IrZHxFcj4u2l/aiIuDUiHiz3R5b2iIgrI2JHRNwbESeN9ieQpLmzKJYk7QU2ZeYJwCnAxRHxMmAzsC0zVwLbyjzAGcDKctsIXDX8yJLUXxbFklS5zNydmV8q008B24GlwFpga+m2FTi7TK8FPpZtdwALI2LJkGNLUl9ZFEuSnhERK4BXAHcCizNzN7QLZ+DY0m0psKtjtfHSJknz1oJRB5AkNUNEPB/4NPCOzPxeREzbtUtbTrPNjbSHWLBo0SJarVYfkvbPxMQErVaLTav3jjoKAIsPpW9Z+vVYTz5GTWKm2TUtDzQzUyeLYkkSEfEc2gXxxzPzM6X50YhYkpm7y/CIPaV9HFjesfoy4JFu283MLcAWgFWrVuXY2Ngg4u+3VqvF2NgYF26+edRRgHZB/L77+vPWvHPdWF+2M/kYNYmZZte0PNDMTJ0cPiFJlYv2KeGrge2Z+Wcdi24E1pfp9cANHe0XlKtQnAI8OTnMQpLmK88US5JOBd4E3BcR95S29wCXAtdGxAbgYeDcsuwW4ExgB/A0cNFw40pS/1kUS1LlMvMLdB8nDHBal/4JXDzQUJI0ZA6fkCRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVz6JYkiRJ1bMoliRJUvUsiiVJklQ9i2JJkiRVb9aiOCI+EhF7IuL+jrajIuLWiHiw3B9Z2iMiroyIHRFxb0ScNMjwkiRJUj/0cqb4L4HTp7RtBrZl5kpgW5kHOANYWW4bgav6E1OSJEkanFmL4sz8e+CxKc1rga1leitwdkf7x7LtDmBhRCzpV1hJkiRpEPZ3TPHizNwNUO6PLe1LgV0d/cZLmyRJktRYC/q8vejSll07RmykPcSCxYsX02q15rTjiYmJOW9jWJqQddPqvT31W3xo731H7UDMOurXCTTj9dqr+ZRVktQs+1sUPxoRSzJzdxkesae0jwPLO/otAx7ptoHM3AJsAVizZk2OjY3tZ5S2VqvFXLcxLE3IeuHmm3vqt2n1Xt53X7//dhqMAzHrznVjgw8ziya8Xns1n7JKkpplfyuIG4H1wKXl/oaO9rdFxDXAq4AnJ4dZSJK0r1b0+Af8/tq0em/PJwkkHdhmLYoj4pPAGHBMRIwDf0C7GL42IjYADwPnlu63AGcCO4CngYsGkFmSJEnqq1mL4sw8f5pFp3Xpm8DFcw0lSZIkDZPfaCdJkqTqzY9PJUmVGvR4ykk7Lz1rKPuRJKmpPFMsSZKk6nmmWJKkA1C//tPUyxU6/G+TDgSeKZYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUPYtiSRIR8ZGI2BMR93e0HRURt0bEg+X+yNIeEXFlROyIiHsj4qTRJZek/rAoliQB/CVw+pS2zcC2zFwJbCvzAGcAK8ttI3DVkDJK0sBYFEuSyMy/Bx6b0rwW2FqmtwJnd7R/LNvuABZGxJLhJJWkwbAoliRNZ3Fm7gYo98eW9qXAro5+46VNkuatBaMOIEmad6JLW3btGLGR9hALFi1aRKvV2qcdbVq9d1+z7ZPFhw5+H/uiaXmgt0z7+rzO1cTExND3OZumZWpaHmhmpk4WxZKk6TwaEUsyc3cZHrGntI8Dyzv6LQMe6baBzNwCbAFYtWpVjo2N7VOACzffvK+Z98mm1Xt5333NeStsWh7oLdPOdWPDCVO0Wi329bU0aE3L1LQ80MxMnRw+IUmazo3A+jK9Hriho/2CchWKU4AnJ4dZSNJ81aw/RyVJIxERnwTGgGMiYhz4A+BS4NqI2AA8DJxbut8CnAnsAJ4GLhp6YEnqM4tiSRKZef40i07r0jeBiwebSJKGy+ETkiRJqp5FsSRJkqrn8IkGWTHgT1lLkiSpO88US5IkqXoWxZIkSaqeRbEkSZKqZ1EsSZKk6lkUS5IkqXoWxZIkSaqeRbEkSZKqZ1EsSZKk6lkUS5IkqXoWxZIkSaqeRbEkSZKqZ1EsSZKk6lkUS5IkqXoLRh1AkiTNbys23zyU/ey89Kyh7Ed18kyxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOpZFEuSJKl6FsWSJEmqnl/eIWnGC+9vWr2XC/t4YX4vvi9JaiLPFEuSJKl6FsWSJEmqnkWxJEmSqmdRLEmSpOr5QbsezPQhpP3R7w8uSZJUg8n342G8j/qh4Pp4pliSJEnVsyiWJElS9SyKJUmSVL2BjCmOiNOB9wMHAR/OzEv7vY+p43wdpyvND/0eo9+p8zjgeMDBG8axXpKGpe9niiPiIOAvgDOAlwHnR8TL+r0fSdLoeKyXdKAZxJnik4EdmfkQQERcA6wFvjaAfUmSRsNjvQ5o+/pfrf39j7X/1WqOQRTFS4FdHfPjwKsGsB9J0uh4rJf6YFBDypo4rLTfmfr9B0VkZn83GHEu8KuZ+ZYy/ybg5Mz87Sn9NgIby+wq4IE57voY4Dtz3MawmHUwzDoYNWV9cWYu6leYA9l+Huv/GXD/UIPOrmmv76blATP1qmmZmpYHmpOp68dUeIkAAAgJSURBVLF+EGeKx4HlHfPLgEemdsrMLcCWfu00Iu7KzDX92t4gmXUwzDoYZtU09vlY38Tnp2mZmpYHzNSrpmVqWh5oZqZOg7gk2z8CKyPiuIh4LnAecOMA9iNJGh2P9ZIOKH0/U5yZeyPibcDnaF+m5yOZ+dV+70eSNDoe6yUdaAZyneLMvAW4ZRDbnkHfhmIMgVkHw6yDYVZ1tR/H+iY+P03L1LQ8YKZeNS1T0/JAMzM9o+8ftJMkSZLmG7/mWZIkSdVrfFEcER+JiD0RcX9H21ERcWtEPFjuj5xm3R9FxD3lNvAPgEyT9dyI+GpE/Dgipv3EZUScHhEPRMSOiNjc8Kw7I+K+8rjeNaKsfxoRX4+IeyPi+ohYOM26TXhce83ahMf135ec90TE5yPihdOsu778/j0YEesbnnWoxwH9tEH8Hu7Le0O0XVn2f29EnNSxTtfXckS8svw+7ijrxiz7WB4Rt0fE9nIcfXsDMh0SEV+MiK+UTH9U2o+LiDtL/09F+4OSRMTBZX5HWb6iY9+XlPYHIuJXZ3tup9tHx/KDIuLLEXFTEzJFl+PviJ+7hRFxXbTfO7ZHxKtHnGdVPHscvScivhcR7xhlpoHIzEbfgF8ETgLu72j7T8DmMr0ZuGyadScakPUE2tdhbgFrplnvIOAbwEuA5wJfAV7WxKyl307gmBE/rq8DFpTpy7q9Bhr0uM6atUGP6ws6pn8H+GCX9Y4CHir3R5bpI5uYtSwb6nHA2089/gP5PdyX9wbgTOCzQACnAHeW9mlfy8AXgVeXdT4LnDHLPpYAJ5Xpw4H/Rfvrr0eZKYDnl+nnAHeWfV0LnFfaPwj8Zpn+rcnfI9pXE/lUmX5Zed4OBo4rz+dBMz230+2j47l6J/AJ4KaZ+g8rE12OvyN+7rYCbynTzwUWjjJPl9/pbwEvbkqmvh2vhnlwnMPBbwU/eeB7AFjScSB6YJr1hv5mODVrR3uL6YviVwOf65i/BLikiVnL8p0MsXibKWtZ9kbg401/XGfK2tDH9RLgqi7t5wMf6pj/EHB+E7OWZRbFI7wN8vew1/eGqa/RyX7TvZbLsq93tD/Tbx/ef24AfqUpmYDnAV+i/a2D3+HZP9SfeX5oX0nk1WV6QekXU5+zyX7TPbdlna77KPPLgG3Aa4CbZuo/xEw7+emieCTPHfAC4H9TPvc16jxdXkuvA/6hSZn6dWv88IlpLM7M3QDl/thp+h0SEXdFxB0Rcfbw4u2zbl+XunREWXqRwOcj4u5of1vVqL2Z9l+VUzXxcZ0uKzTkcY2IP4mIXcA64Pe7dGnM49pDVpg/x4ED1TBfL9O9N0yXYab28Wkyz/r+U/7F/wraZ2ZHminawxTuAfYAt9I+i/pEZu7tsp1n9l2WPwkcvR9Zj55hHwBXAO8CflzmZ+o/rEzdjr+jeu5eAnwb+Gi0h5h8OCIOG2Geqc4DPjlL/2Fn6ov5WhT36kXZ/uaU3wCuiIifG3WgaUSXthx6it6dmpknAWcAF0fEL44qSES8F9gLfLzb4i5tI3tcZ8kKDXlcM/O9mbmcds63denSmMe1h6wwf44DB6omvF6my7Cv7bPvKOL5wKeBd2Tm90adKTN/lJkn0j47ezLtYXLTbadfmabNGhGvB/Zk5t0dy2b62QaeqdiX4++gn7sFtIcGXZWZrwC+T3vYwKjyPLuj9jjsNwD/dbauw8rUT/O1KH40IpYAlPs93Tpl5iPl/iHaQwJeMayA+6inr0ttio7HdQ9wPe0D7dCVAfqvB9Zl+b/KFI15XHvI2pjHtcMngF/r0t6Yx7XDdFnn03HgQDXM18t07w3TZZipfdk0mad9/4mI59AuiD+emZ9pQqZJmfkE7df/KcDCiFjQZTvP7LssPwJ4bD+yfmeGfZwKvCEidgLX0B5CccWIM013/B3VczcOjGfmnaXPdbSL5Ca8ls4AvpSZj87Sf6iv736Zr0XxjcD6Mr2e9titnxARR0bEwWX6GNq/iF8bWsJ9M2++LjUiDouIwyenaY8tun/mtQaS43Tg3cAbMvPpabo14nHtJWuDHteVHbNvAL7epdvngNeV37EjaWf93DDydeol6zw7Dhyohvl7ON17w43ABeUT8acAT5Z/w3Z9LZdlT0XEKeUT8BdM2dZP7aP0uxrYnpl/1pBMi6Jc7SYiDgVeC2wHbgfOmSbT5HbOAW4rf8TfCJwX7StBHAespP2hqK7PbVmn6z4y85LMXJaZK0r/2zJz3SgzzXD8Hclzl5nfAnZFxKrSfhrt49bIXksdzufZoRMz9R9mpv4Z1GDlft1oP/i7gf9H+y+JDbTHBm0DHiz3R5W+a4APl+l/AdxH+5On9wEbRpT1jWX6h8CjPPvhgRcCt3SseybtTyt/A3hvU7PSHuv0lXL76giz7qA9Lumecvvg1KwNelxnzdqgx/XTtN8M7gX+Blha+j7zu1Xm31x+rh3ARU3NOorjgLeuz1/ffw+neU1M994QwF+U/d9HxweJp3stl9fR/WWdD8AzX3Y13T7+Je1/997b8bt+5ogz/QLw5ZLpfuD3S/tLaBeQO2j/G/zg0n5Imd9Rlr+kY9/vLft9gHJVgJme2+n2MeU5HOPZq0+MLBPTHH9H/NydCNxVnru/pn2lhpHlKcueB3wXOKKjbaSZ+n3zG+0kSZJUvfk6fEKSJEnqG4tiSZIkVc+iWJIkSdWzKJYkSVL1LIolSZJUPYtiSZIkVc+iWJIkSdWzKJYkSVL1/j8Aa9hVG4FztQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concat train/test en all_data\n",
    "\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "\n",
    "# Taille figure \n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Application d'une fonction log sur Price (+1 pour eviter les nan/zro)\n",
    "\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "prices.hist()\n",
    "\n",
    "# Remplace log.Price sur le dataset all_data\n",
    "\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# Sortir le nombre de catgorielles du all_data\n",
    "\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Pour normaliser les \"skewed features\"\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "\n",
    "# all_data ajout des skewed_feats\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# get_dummies pour avoir les colonnes catgorielles.\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# S'occuper des NA du all_data par la moyenne du tableau\n",
    "\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "# Cration du X_train, X_test et y\n",
    "\n",
    "X_train = all_data[:train.shape[1]]\n",
    "X_test = all_data[train.shape[1]:]\n",
    "y = train.SalePrice\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostLibraryNotFound",
     "evalue": "Cannot find XGBoost Library in the candidate path, did you install compilers and run build.sh in root path?\nList of candidates:\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\../../lib/xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\./lib/xgboost.dll\nC:\\Users\\utilisateur\\Anaconda3\\xgboost\\xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\../../windows/x64/Release/xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\./windows/x64/Release/xgboost.dll",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostLibraryNotFound\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef52a05c5e6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import XGBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrabit\u001b[0m                   \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;31m# load the XGBoost library globally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m \u001b[0m_LIB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;34m\"\"\"Load xgboost Library.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mlib_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_lib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\libpath.py\u001b[0m in \u001b[0;36mfind_lib_path\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;34m'Cannot find XGBoost Library in the candidate path, '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;34m'did you install compilers and run build.sh in root path?\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             'List of candidates:\\n' + ('\\n'.join(dll_path)))\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostLibraryNotFound\u001b[0m: Cannot find XGBoost Library in the candidate path, did you install compilers and run build.sh in root path?\nList of candidates:\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\../../lib/xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\./lib/xgboost.dll\nC:\\Users\\utilisateur\\Anaconda3\\xgboost\\xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\../../windows/x64/Release/xgboost.dll\nC:\\Users\\utilisateur\\AppData\\Roaming\\Python\\Python37\\site-packages\\xgboost\\./windows/x64/Release/xgboost.dll"
     ]
    }
   ],
   "source": [
    "# Import XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label = y)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"max_depth\":2, \"eta\":0.1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n",
    "\n",
    "model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\n",
    "model_xgb.fit(X_train, y)\n",
    "\n",
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
    "\n",
    "predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\n",
    "predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")\n",
    "\n",
    "preds = 0.7*lasso_preds + 0.3*xgb_preds\n",
    "\n",
    "solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\n",
    "solution.to_csv(\"ridge_sol.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/49/b95c037b717b4ceadc76b6e164603471225c27052d1611d5a2e832757945/xgboost-0.90-py2.py3-none-win_amd64.whl (18.3MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from xgboost) (1.16.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from xgboost) (1.3.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
